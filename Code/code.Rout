
R version 4.1.2 (2021-11-01) -- "Bird Hippie"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ################################################################################
> ################################################################################
> # Replication material for "Estimating conditional distributions with Neural
> # Networks using R package deeptrafo" by Lucas Kook, Philipp FM Baumann, Oliver
> # Duerr, Beate Sick, and David Ruegamer.
> ################################################################################
> ################################################################################
> 
> # Dependencies can be installed via the separately provided `dependencies.R`
> # file
> 
> ## ----include=FALSE------------------------------------------------------------
> library("knitr")
> opts_chunk$set(engine = "R", tidy = FALSE, prompt = TRUE, cache = FALSE,
+                fig.width = 6.5 * 0.8, fig.height = 4.5 * 0.8,
+                fig.fullwidth = TRUE, fig.path = "Figures/",
+                fig.ext = c("pdf", "eps", "jpg", "tiff"), dpi = 600)
> knitr::render_sweave()  # use Sweave environments
> knitr::set_header(highlight = "")  # do not \usepackage{Sweave}
> ## R settings
> options(prompt = "R> ", continue = "+  ", width = 76, useFancyQuotes = FALSE,
+         digits = 3L)
R> 
R> ## ----preliminaries, echo = FALSE, results = "hide", message=FALSE-------------
R> library("deeptrafo")
Loading required package: tensorflow
Loading required package: keras
Loading required package: tfprobability
Loading required package: deepregression
R> library("ggplot2")
R> library("tsdl") # available from GitHub (FinYang/tsdl)
R> library("reticulate")
R> library("safareg")
R> library("data.table")
R> library("patchwork")
R> library("ggridges")
R> library("moments")
R> library("lubridate")

Attaching package: 'lubridate'

The following objects are masked from 'package:data.table':

    hour, isoweek, mday, minute, month, quarter, second, wday,
    week, yday, year

The following objects are masked from 'package:base':

    date, intersect, setdiff, union

R> library("boot")
R> library("ggrepel")
R> library("tram")
Loading required package: mlt
Loading required package: basefun
Loading required package: variables

Attaching package: 'variables'

The following object is masked from 'package:ggplot2':

    unit

Loading required package: mvtnorm
R> 
R> theme_set(theme_bw() + theme(legend.position = "top"))
R> 
R> outdir <- "../Figures"
R> if (!dir.exists(outdir)) dir.create(outdir)
R> 
R> # Params ------------------------------------------------------------------
R> 
R> bpath <- "../Data"
R> nr_words <- 1e4
R> embedding_size <- 1e2
R> maxlen <- 1e2
R> order_bsp <- 25
R> repetitions <- 2
R> 
R> # Loading the data --------------------------------------------------------
R> if (!file.exists("data_splitted.RDS"))
+    source("movies.R")
R> 
R> ATMonly <- FALSE
R> 
R> ## ----data, eval=!ATMonly------------------------------------------------------
R> data_list <- readRDS(file.path(bpath, "data_splitted.RDS"))[[1]]
R> train <- data_list[[1]]
R> test <- data_list[[2]]
R> 
R> tokenizer <- text_tokenizer(num_words = nr_words)
2024-01-09 10:05:31.502291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-09 10:05:31.688770: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server
2024-01-09 10:05:31.688809: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2024-01-09 10:05:31.714999: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-09 10:05:32.260498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server
2024-01-09 10:05:32.260573: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server
2024-01-09 10:05:32.260582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
R> mov <- readRDS(file.path(bpath, "mov_ov.RDS"))
R> tokenizer |> fit_text_tokenizer(mov)
R> words <- readRDS(file.path(bpath, "words.RDS"))
R> 
R> ggplot(data.frame(vote_count = train$vote_count), aes(x = vote_count)) +
+    stat_ecdf() +
+    geom_vline(xintercept = test$vote_count[1], color = "darkblue", linetype = 2) +
+    labs(y = "ECDF (training data)") +
+    theme(text = element_text(size = 13.5))
R> 
R> ggsave(file.path(outdir, "vote-count.pdf"), width = 4, height = 3)
R> 
R> ## ----formula-interface, eval=!ATMonly-----------------------------------------
R> fm <- vote_count | genreAction ~ 0 + s(budget, df = 6) + popularity
R> 
R> 
R> ## ----setting-up-dctms, eval=!ATMonly------------------------------------------
R> opt <- optimizer_adam(learning_rate = 0.1, decay = 4e-4)
R> (m_fm <- cotramNN(formula = fm, data = train, optimizer = opt))
2024-01-09 10:05:33.808811: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server:/usr/lib/R/lib:/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server
2024-01-09 10:05:33.808853: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)
2024-01-09 10:05:33.808871: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lucas-cx1): /proc/driver/nvidia/version does not exist
2024-01-09 10:05:33.809121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
	 Untrained count outcome deep conditional transformation model

Call:
cotramNN(formula = fm, data = train, optimizer = opt)

Interacting:  vote_count | genreAction 

Shifting:  ~0 + s(budget, df = 6) + popularity 

Shift coefficients:
s(budget, df = 6)1 s(budget, df = 6)2 s(budget, df = 6)3 s(budget, df = 6)4 
           -0.4760            -0.7326            -0.6233            -0.4061 
s(budget, df = 6)5 s(budget, df = 6)6 s(budget, df = 6)7 s(budget, df = 6)8 
           -0.4309            -0.5447             0.6729             0.7376 
s(budget, df = 6)9         popularity 
            0.0947             1.2447 
R> 
R> 
R> ## ----fitting-dctms, eval=!ATMonly---------------------------------------------
R> m_fm_hist <- fit(m_fm, epochs = 1e3, validation_split = 0.1, batch_size = 64,
+                   verbose = FALSE)
R> unlist(coef(m_fm, which = "shifting"))
s(budget, df = 6)1 s(budget, df = 6)2 s(budget, df = 6)3 s(budget, df = 6)4 
           0.38557           -0.29035           -0.04621           -0.04052 
s(budget, df = 6)5 s(budget, df = 6)6 s(budget, df = 6)7 s(budget, df = 6)8 
           0.00610           -0.02743           -0.00526            0.01523 
s(budget, df = 6)9         popularity 
          -0.36896           -0.83196 
R> 
R> 
R> ## ----plot-hist, fig.width=8.1, fig.height=4.05, eval=!ATMonly-----------------
R> p1 <- data.frame(x = 1:m_fm_hist$params$epochs,
+                   training = m_fm_hist$metrics$loss,
+                   validation = m_fm_hist$metrics$val_loss) |>
+    tidyr::gather("set", "loss", training, validation) |>
+    ggplot(aes(x = x, y = loss, color = set)) +
+    geom_line() +
+    scale_color_brewer(palette = "Dark2") +
+    labs(x = "epochs", color = "") +
+    theme(text = element_text(size = 13))
R> 
R> nd <- list(genreAction = c(0, 1), budget = rep(mean(train$budget), 2),
+             popularity = rep(mean(train$popularity), 2))
R> preds <- predict(m_fm, newdata = nd, q = seq(0, 2000, 25), type = "trafo")
R> pdat <- data.frame(y = as.numeric(names(preds)),
+                     ga0 = unlist(lapply(preds, \(x) x[1, 1])),
+                     ga1 = unlist(lapply(preds, \(x) x[2, 1])))
R> p2 <- pdat |> tidyr::gather("action", "trafo", ga0, ga1) |>
+    ggplot(aes(x = y, y = trafo, color = action)) +
+    geom_step() +
+    labs(x = "vote count", y = "transformation function", color = "genreAction") +
+    scale_color_manual(values = colorspace::diverging_hcl(2),
+                       labels = c("ga0" = 0, "ga1" = 1)) +
+    theme(text = element_text(size = 13))
R> 
R> ggpubr::ggarrange(
+    p1 + labs(tag = "A"), p2 + labs(tag = "B"), nrow = 1,
+    common.legend = FALSE
+  )
R> 
R> ggsave(file.path(outdir, "plot-hist.pdf"), width = 8.1, height = 4.05)
R> 
R> ## ----working-with-neural-networks, eval=!ATMonly------------------------------
R> embd_mod <- function(x) x |>
+    layer_embedding(input_dim = nr_words, output_dim = embedding_size) |>
+    layer_lstm(units = 50, return_sequences = TRUE) |>
+    layer_lstm(units = 50, return_sequences = FALSE) |>
+    layer_dropout(rate = 0.1) |>
+    layer_dense(25) |>
+    layer_dropout(rate = 0.2) |>
+    layer_dense(5) |>
+    layer_dropout(rate = 0.3) |>
+    layer_dense(1)
R> 
R> fm_deep <- update(fm, . ~ . + deep(texts))
R> m_deep <- deeptrafo(fm_deep, data = train,
+                      list_of_deep_models = list(deep = embd_mod))
R> 
R> fit(m_deep, epochs = 50, validation_split = 0.1, batch_size = 32,
+      callbacks = list(callback_early_stopping(patience = 5)), verbose = FALSE)
R> 
R> 
R> ## ----ensembling-dctms, eval=!ATMonly------------------------------------------
R> ens_deep <- ensemble(m_deep, n_ensemble = 3, epochs = 50, batch_size = 64,
+                       verbose = FALSE)
Fitting member 1 ...
Done in 5.34  mins 
Fitting member 2 ...
Done in 5.54  mins 
Fitting member 3 ...
Done in 5.41  mins 
R> 
R> 
R> ## ----ensembling-dctms-methods, eval=!ATMonly----------------------------------
R> unlist(logLik(ens_deep, newdata = test, convert_fun = \(x) - mean(x)))
members1 members2 members3     mean ensemble 
   -8.83    -9.06    -8.95    -8.94    -8.92 
R> 
R> ## ----ensembling-dctms-plot-prereq, eval=!ATMonly------------------------------
R> d <- deeptrafo:::.call_for_all_members(
+    ens_deep, deeptrafo:::plot.deeptrafo, only_data = TRUE)
R> pd <- data.frame(cbind(
+    value = d[[1]][[1]]$value, do.call("cbind", lapply(
+      d, \(x) x[[1]]$partial_effect))))
R> ord <- order(pd$value)
R> pdat <- pd[ord,]
R> nd <- data.frame(
+    value = pdat$value, ens = apply(pdat[, -1], 1, mean),
+    sd = apply(pdat[, -1], 1, sd))
R> 
R> 
R> ## ----ensembling-dctms-plot, eval=!ATMonly-------------------------------------
R> pdf(file.path(outdir, "ensembling-dctms-plot.pdf"), height = 4.5 * 0.8, width = 6.5 * 0.8)
R> plot(pdat$value, pdat$V2, type = "l", ylim = range(pdat[, -1]),
+       xlab = "log(1 + bugdet)", ylab = "partial effect", col = "gray80", lty = 2,
+       las = 1)
R> matlines(pdat$value, pdat[, -1:-2], col = "gray80", lty = 2)
R> rug(train$budget, col = rgb(.1, .1, .1, .3))
R> 
R> lines(nd$value, nd$ens)
R> polygon(c(nd$value, rev(nd$value)),
+          c(nd$ens + 2 * nd$sd, rev(nd$ens - 2 * nd$sd)),
+          col = rgb(.1, .1, .1, .1), border = FALSE)
R> 
R> legend("topright", legend = c("ensemble", "individual"), lty = c(1, 2),
+         col = c(1, "gray80"), lwd = 1, bty = "n")
R> dev.off()
pdf 
  2 
R> 
R> 
R> ## ----cross-validating-dctms, fig.width=9, fig.height=4.05, eval=!ATMonly------
R> cv_deep <- cv(m_deep, epochs = 50, cv_folds = 5, batch_size = 64)
Fitting Fold  1  ... 
Done in 4.98  mins 
Fitting Fold  2  ... 
Done in 4.78  mins 
Fitting Fold  3  ... 
Done in 4.29  mins 
Fitting Fold  4  ... 
Done in 4.14  mins 
Fitting Fold  5  ... 
Done in 4.48  mins 
R> pdf(file.path(outdir, "cross-validating-dctms.pdf"), height = 4.5, width = 9)
R> plot_cv(cv_deep)
R> dev.off()
pdf 
  2 
R> 
R> 
R> ## ----preproc-ontram-----------------------------------------------------------
R> train$action <- ordered(train$genreAction)
R> test$action <- ordered(test$genreAction, levels = levels(train$action))
R> 
R> 
R> ## ----embd_mod-----------------------------------------------------------------
R> make_keras_model <- function() {
+    return(
+      keras_model_sequential(name = "embd")  |>
+        layer_embedding(input_dim = nr_words, output_dim = embedding_size) |>
+        layer_lstm(units = 50, return_sequences = TRUE)  |>
+        layer_lstm(units = 50, return_sequences = FALSE)  |>
+        layer_dropout(rate = 0.1) |>
+        layer_dense(25)  |>
+        layer_dropout(rate = 0.2)  |>
+        layer_dense(5, name = "penultimate")  |>
+        layer_dropout(rate = 0.3) |>
+        layer_dense(1)
+    )
+  }
R> 
R> 
R> ## ----formulas-ontram----------------------------------------------------------
R> fm_0 <- action ~ 1
R> fm_tab <- action ~ 0 + popularity
R> fm_text <- action ~ 0 + deep(texts)
R> fm_semi <- action ~ 0 + popularity + deep(texts)
R> 
R> 
R> ## ----mods-ontram--------------------------------------------------------------
R> ### unconditional ####
R> m_0 <- PolrNN(fm_0, data = train, optimizer = optimizer_adam(
+    learning_rate = 1e-2, decay = 1e-4), weight_options = weight_control(
+      general_weight_options = list(trainable = FALSE, use_bias = FALSE),
+      warmstart_weights = list(list(), list(), list("1" = 0))))
R> fit(m_0, epochs = 3e3, validation_split = 0, batch_size = length(
+    train$action), verbose = FALSE)
R> 
R> all.equal(unlist(unname(coef(m_0, which = "interacting"))),
+    qlogis(mean(train$action == 0)), tol = 1e-6)
[1] TRUE
R> 
R> ### only tabular ####
R> m_tab <- PolrNN(fm_tab, data = train, optimizer = optimizer_adam(
+    learning_rate = 0.1, decay = 1e-4))
R> fit(m_tab, epochs = 1e3, batch_size = length(train$action),
+      validation_split = 0, verbose = FALSE)
R> exp(-unlist(coef(m_tab, which = "shifting")))
popularity 
      1.54 
R> 
R> ### only text ####
R> embd <- make_keras_model()
R> m_text <- PolrNN(fm_text, data = train, list_of_deep_models = list(
+    deep = embd), optimizer = optimizer_adam(learning_rate = 1e-4))
R> fit(m_text, epochs = 10, callbacks = list(callback_early_stopping(
+    patience = 2, restore_best_weights = TRUE)), verbose = FALSE)
R> 
R> ### semi-structured ####
R> embd_semi <- make_keras_model()
R> 
R> optimizer <- function(model) {
+    optimizers_and_layers <- list(
+      tuple(optimizer_adam(learning_rate = 1e-2),
+            get_layer(model, "ia_1__2")),
+      tuple(optimizer_adam(learning_rate = 1e-2),
+            get_layer(model, "popularity_3")),
+      tuple(optimizer_adam(learning_rate = 1e-4),
+            get_layer(model, "embd")))
+    multioptimizer(optimizers_and_layers)
+  }
R> m_semi <- PolrNN(fm_semi, data = train, list_of_deep_models = list(
+    deep = embd_semi), optimizer = optimizer)
R> fit(m_semi, epochs = 10, callbacks = list(callback_early_stopping(
+    patience = 2, restore_best_weights = TRUE)), verbose = FALSE)
R> 
R> 
R> ## ----mods-comparison-ontram---------------------------------------------------
R> all.equal(unname(unlist(coef(m_0, which = "interacting"))),
+            qlogis(mean(train$action == 0)), tol = 1e-6)
[1] TRUE
R> 
R> # compare test prediction performance
R> bci <- function(mod) {
+    lli <- logLik(mod, newdata = test, convert_fun = identity)
+    bt <- boot(lli, statistic = \(x, d) mean(x[d]), R = 1e4)
+    btci <- boot.ci(bt, conf = 0.95, type = "perc")$percent[1, 4:5]
+    c("nll" = mean(lli), "lwr" = btci[1], "upr" = btci[2])
+  }
R> 
R> mods <- list("unconditional" = m_0, "tabular only" = m_tab,
+               "text only" = m_text, "semi-structured" = m_semi)
R> do.call("cbind", lapply(mods, bci))
    unconditional tabular only text only semi-structured
nll         0.531        0.516     0.435           0.422
lwr         0.501        0.486     0.389           0.371
upr         0.562        0.549     0.483           0.477
R> 
R> c("tabular only" = unlist(unname(coef(m_tab))),
+    "semi-structured" = unlist(unname(coef(m_semi))))
   tabular only semi-structured 
         -0.430          -0.315 
R> 
R> 
R> ## ----embedding-pca-plot-------------------------------------------------------
R> d <- table(test$texts)
R> d_sorted <- d[order(d, decreasing = TRUE)]
R> nw <- as.integer(names(d_sorted[1:1e3]))
R> 
R> mod <- keras_model(embd$layers[[1]]$input, get_layer(
+    embd, "embedding_1")$output)
R> number_words_per_movie <- 100
R> inp <- texts_to_sequences(tokenizer, words$word[nw]) |>
+    pad_sequences(maxlen = number_words_per_movie, truncating = "post")
R> dat <- mod(inp)$numpy()
R> 
R> dat <- cbind(words[nw, ], dat)
R> res <- prcomp(dat[,-c(1:2)])
R> 
R> ### ggplot word embedding ###
R> df <- data.frame(
+    PC1 = res$x[,1],
+    PC2 = res$x[,2],
+    words = dat$word
+  )
R> 
R> topN <- 100
R> 
R> gp1 <- ggplot(df, aes(x = PC1, y = PC2)) + geom_point(size = 0.2, col = "gray") +
+    geom_point(data = df[1:topN, ], aes(x = PC1, y = PC2), size = 0.2, col = "black") +
+    geom_text_repel(data = df[1:topN, ], aes(x = PC1, y = PC2, label = words),
+                    size = rel(3.5), max.overlaps = 20) +
+    theme_bw() + labs(subtitle = paste0(topN, " most frequent words")) +
+    theme(text = element_text(size = 13))
R> 
R> ##### PCA of embedding from m_text ####
R> mod <- keras_model(embd$layers[[1]]$input, get_layer(
+    embd, "penultimate")$output)
R> mod_movie_dat <- mod(test$texts)$numpy()
R> res <- prcomp(mod_movie_dat)
R> 
R> genres <- rep(0, length(test$genreAction)) # All movies
R> genres[test$genreAction == 1] <- 1 # Action Movis
R> genres[test$genreRomance == 1] <- 2 # Action Romance
R> 
R> ### ggplot text embedding ###
R> df2 <- data.frame(
+    PC1 = res$x[,1],
+    PC2 = res$x[,2],
+    genres = factor(genres, levels = 0:2, labels = c(
+      "Neither", "Action", "Romance"))
+  )
R> 
R> gp2 <- ggplot(df2, aes(x = PC1, y = PC2, col = genres, size = genres)) +
+    geom_point() +
+    scale_color_manual(values = c("gray", "red", "blue")) +
+    scale_size_manual(values = 1.3 * c(0.3, 1, 1)) +
+    theme_bw() + theme(legend.position = "top") +
+    labs(subtitle = "movie reviews in the test data") +
+    theme(text = element_text(size = 13))
R> 
R> ggpubr::ggarrange(gp1, gp2, common.legend = TRUE)
Warning message:
ggrepel: 2 unlabeled data points (too many overlaps). Consider increasing max.overlaps 
R> ggsave(file = file.path(outdir, "embedding-pca.pdf"), height = 4, width = 8)
Warning message:
ggrepel: 14 unlabeled data points (too many overlaps). Consider increasing max.overlaps 
R> 
R> 
R> ## ----preliminaries_ATM, echo = FALSE, results = "hide", message=FALSE---------
R> d <- subset(tsdl, "Meteorology")
R> nm <- "Mean maximum temperature in Melbourne: degrees C. Jan 71 – Dec 90."
R> temp_idx <- sapply(d, attr, "description") == nm
R> y <- d_ts <- d[temp_idx][[1]]
R> 
R> n_gr <- 300
R> min_supp <- 10
R> max_supp <- 30
R> M <- 6L # order bsp
R> ep <- 1e4
R> p <- 3
R> gr <- seq(min_supp ,max_supp , length.out = n_gr)
R> time_var <- seq(as.Date("1971-01-01"), as.Date("1990-12-01"), by = "month")
R> d_ts <- data.table(time = time_var, y = as.numeric(d_ts))
R> d_ts[, month := factor(month(time))]
R> d_ts_lag <- d_ts[, paste0("y_lag_", 1:p) := shift(y, n = 1:p, type = "lag", fill = NA)]
R> d_ts_lag <- na.omit(d_ts_lag)
R> 
R> ## ----formula-interface_ATM----------------------------------------------------
R> lags <- c(paste0("y_lag_", 1:p, collapse = "+"))
R> (fm_atm <- as.formula(paste0("y |", lags, "~ 0 + month + atplag(1:p)")))
y | y_lag_1 + y_lag_2 + y_lag_3 ~ 0 + month + atplag(1:p)
R> (fm_atp <- y ~ 0 + month + atplag(1:p))
y ~ 0 + month + atplag(1:p)
R> (fm_colr <- as.formula(paste0("y ~ 0 + month + ", lags)))
y ~ 0 + month + y_lag_1 + y_lag_2 + y_lag_3
R> 
R> 
R> ## ----fitting-ATMs-------------------------------------------------------------
R> mod_fun <- function(fm, d) ColrNN(fm, data = d, trafo_options = trafo_control(
+    order_bsp = M, support = c(min_supp, max_supp)), tf_seed = 1,
+    optimizer = optimizer_adam(learning_rate = 0.01))
R> 
R> mods <- c(lapply(list(fm_atm, fm_atp), mod_fun, d = d_ts),
+            lapply(list(fm_colr), mod_fun, d = d_ts_lag))
R> 
R> fit_fun <- \(m) m |> fit(epochs = ep, callbacks = list(
+    callback_early_stopping(patience = 50, monitor = "loss"),
+    callback_reduce_lr_on_plateau(patience = 20, factor = 0.9, monitor = "loss")),
+    batch_size = nrow(d_ts_lag), validation_split = 0, verbose = FALSE)
R> 
R> lapply(mods, \(m) {
+    mhist <- fit_fun(m)
+    # plot(mhist)
+  })
[[1]]

Final epoch (plot to see history):
loss: 1.47
  lr: 0.000008595 

[[2]]

Final epoch (plot to see history):
loss: 1.613
  lr: 0.000002185 

[[3]]

Final epoch (plot to see history):
loss: 1.486
  lr: 0.00000177 

R> 
R> 
R> ## ----in-sample-logLiks-ATM----------------------------------------------------
R> t_span_one <- seq(as.Date("1977-03-01"), as.Date("1978-05-01"), by = "month")
R> ndl <- d_ts[d_ts$time %in% t_span_one]
R> t_span_two <- seq(as.Date("1977-06-01"), as.Date("1978-05-01"), by = "month")
R> ndl_lag <- d_ts_lag[d_ts_lag$time %in% t_span_two]
R> structure(unlist(c(lapply(mods[1:2], logLik, newdata = ndl),
+                     lapply(mods[3], logLik, newdata = ndl_lag))), names = c(
+    "ATM", paste0("AT(", p, ")"), "Colr"))
  ATM AT(3)  Colr 
-19.2 -22.5 -20.0 
R> 
R> 
R> ## ----in_sample_cond_densities_ATMs--------------------------------------------
R> # In-sample densities
R> m_atm <- mods[[1]]
R> m_atp <- mods[[2]]
R> m_colr <- mods[[3]]
R> 
R> # colr
R> nd <- ndt <- d_ts_lag |> dplyr::mutate(y_true = y, y = list(gr)) |>
+    tidyr::unnest(y)
R> nd$d_colr <- c(predict(m_colr, newdata = nd, type = "pdf"))
R> 
R> pg <- TRUE
R> attr(pg, "rname") <- "y_true"
R> attr(pg, "y") <- "y"
R> 
R> # atp and atm
R> nd_atp <- ndt_atp <- d_ts |> dplyr::mutate(y_true = y, y = list(gr)) |>
+    tidyr::unnest(y)
R> nd_atp_lag <- subset(nd_atp, time >= "1971-04-01")
R> nd_atp <- nd_atp[order(nd_atp$y, nd_atp$time),]
R> nd_atp_lag <- nd_atp_lag[order(nd_atp_lag$y, nd_atp_lag$time),]
R> nd_atp_lag$d_atp <- c(predict(m_atp, newdata = nd_atp, type = "pdf",
+                                pred_grid = pg))
R> nd_atp_lag$d_atm <- c(predict(m_atm, newdata = nd_atp, type = "pdf",
+                                pred_grid = pg))
R> 
R> nd <- merge(nd, nd_atp_lag)
R> 
R> d_density <- nd |>
+    tidyr::gather("method", "y_density", d_atm, d_atp, d_colr) |>
+    dplyr::mutate(y_grid = y) |> as.data.table()
R> 
R> # In-sample trafos
R> 
R> # colr
R> ndt$t_colr <- c(predict(m_colr, newdata = ndt, type = "trafo"))
R> 
R> # atp and atm
R> ndt_atp_lag <- subset(ndt_atp, time >= "1971-04-01")
R> ndt_atp <- ndt_atp[order(ndt_atp$y, ndt_atp$time),]
R> ndt_atp_lag <- nd_atp_lag[order(nd_atp_lag$y, nd_atp_lag$time),]
R> ndt_atp_lag$t_atp <- c(predict(m_atp, newdata = ndt_atp, type = "trafo",
+                                 pred_grid = pg))
R> ndt_atp_lag$t_atm <- c(predict(m_atm, newdata = ndt_atp, type = "trafo",
+                                 pred_grid = pg))
R> 
R> ndt <- merge(ndt, ndt_atp_lag)
R> 
R> ## ----plot-ATMs, fig.width=10.8, fig.height=8.1--------------------------------
R> plot_period <- seq(as.Date("1983-06-01"), as.Date("1984-05-01"), by = "month")
R> d_sub_dens <- d_density[time %in% plot_period]
R> 
R> Sys.setlocale("LC_ALL", "en_GB.UTF-8")
[1] "LC_CTYPE=en_GB.UTF-8;LC_NUMERIC=C;LC_TIME=en_GB.UTF-8;LC_COLLATE=en_GB.UTF-8;LC_MONETARY=en_GB.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=de_CH.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=de_CH.UTF-8;LC_IDENTIFICATION=C"
R> g_dens <- ggplot() +
+    geom_path(data = d_sub_dens, aes(x = y_true, y = time, group = method),
+              colour="red", linewidth = 1.5, alpha = 0.2) +
+    geom_point(data = d_sub_dens, aes(x = y_true, y = time, group = method),
+               colour="red", size = 1, shape = 4) +
+    geom_density_ridges(data = d_sub_dens,
+             aes(height = y_density, x = y_grid, y = time,
+                 group = time, fill = factor(time)),
+             stat = "identity", alpha = 0.7, colour = rgb(0, 0, 0, 0.5)) +
+    scale_y_date(date_breaks = "4 months", date_labels = "%b %Y") +
+    scale_fill_viridis_d() +
+    guides(fill = guide_legend(nrow = 2)) +
+    facet_grid(~ method, labeller = as_labeller(c("d_atp" = paste0("AT(", p, ")"),
+                                                  "d_atm" = "ATM",
+                                                  "d_colr" = "Colr"))) +
+    theme_bw() +
+    labs(color = "month") +
+    xlab("") +
+    ylab("") +
+    theme(panel.grid.major = element_blank(),
+          panel.grid.minor = element_blank(),
+          strip.background = element_blank(),
+          panel.border = element_blank(),
+          text = element_text(size = 13),
+          legend.position = "none",
+          rect = element_rect(fill = "transparent"))
R> 
R> trafos <- ndt |>
+    tidyr::gather("method", "h", t_atm, t_atp, t_colr) |>
+    dplyr::filter(time %in% plot_period) |>
+    dplyr::mutate(month = ordered(
+      format(as.Date(time), format = "%b %Y"),
+      levels = format(sort(unique(as.Date(time))), format = "%b %Y")))
R> 
R> g_trafo <- ggplot(trafos) +
+    geom_line(aes(x = y, y = h, color = month)) +
+    theme_set(theme_bw() + theme(legend.position = "bottom")) +
+    facet_grid(~ method, labeller = as_labeller(
+      c("t_atm" = "ATM", "t_atp" = paste0("AT(", p, ")"), "t_colr" = "Colr"))) +
+    ylab(expression(hat(h)(y[t]*"|"*Y[p] *"="* y[p]))) +
+    xlab(expression(y[t])) +
+    scale_x_continuous(expand = c(0,0), breaks = c(15, 20, 25)) +
+    guides(color = guide_legend(nrow = 2)) +
+    theme(text = element_text(size = 13), legend.position = "bottom") +
+    xlab("Mean maximum temperature (°C) in Melbourne") +
+    theme(strip.background = element_blank(), strip.text.x = element_blank()) +
+    scale_colour_viridis_d()
R> 
R> (p1 <- g_dens / g_trafo)
R> 
R> ggsave(file.path(outdir, "plot-ATMs.pdf"), p1, width = 10.8, height = 8.1)
R> 
R> ## ----handling-censored-responses, eval=!ATMonly-------------------------------
R> deeptrafo:::response(y = c(0L, 1L))
     cleft exact cright cinterval
[1,]     1     0      0         0
[2,]     0     0      0         1
attr(,"type")
[1] "count"
R> 
R> 
R> ## ----warmstart-and-fix-weights-shift, eval=!ATMonly---------------------------
R> nn <- keras_model_sequential() |>
+    layer_dense(input_shape = 1L, units = 3L, activation = "relu",
+                use_bias = FALSE, kernel_initializer = initializer_constant(value = 1))
R> unlist(get_weights(nn))
[1] 1 1 1
R> 
R> 
R> ## ----weight_control, eval=!ATMonly--------------------------------------------
R> args(weight_control)
function (specific_weight_options = NULL, general_weight_options = list(activation = NULL, 
    use_bias = FALSE, trainable = TRUE, kernel_initializer = "glorot_uniform", 
    bias_initializer = "zeros", kernel_regularizer = NULL, bias_regularizer = NULL, 
    activity_regularizer = NULL, kernel_constraint = NULL, bias_constraint = NULL), 
    warmstart_weights = NULL, shared_layers = NULL) 
NULL
R> 
R> 
R> ## ----warmstart-and-fix-weights-shift-2, eval=!ATMonly-------------------------
R> data("wine", package = "ordinal")
R> mw <- deeptrafo(rating ~ 0 + temp, data = wine, weight_options = weight_control(
+    warmstart_weights = list(list(), list(), list("temp" = 0))))
R> unlist(coef(mw))
temp 
   0 
R> 
R> ## ----custom-basis, eval=!ATMonly----------------------------------------------
R> linear_basis <- function(y) {
+    ret <- cbind(1, y)
+    if (NROW(ret) == 1)
+      return(as.vector(ret))
+    ret
+  }
R> 
R> linear_basis_prime <- function(y) {
+    ret <- cbind(0, rep(1, length(y)))
+    if (NROW(ret) == 1)
+      return(as.vector(ret))
+    ret
+  }
R> 
R> constraint <- function(w, bsp_dim) {
+    w_res <- tf$reshape(w, shape = list(bsp_dim, as.integer(nrow(w) / bsp_dim)))
+    w1 <- tf$slice(w_res, c(0L, 0L), size = c(1L, ncol(w_res)))
+    wrest <- tf$math$softplus(tf$slice(w_res, c(1L, 0L), size = c(as.integer(nrow(
+      w_res) - 1), ncol(w_res))))
+    w_w_cons <- k_concatenate(list(w1, wrest), axis = 1L)
+    tf$reshape(w_w_cons, shape = list(nrow(w), 1L))
+  }
R> 
R> tfc <- trafo_control(
+    order_bsp = 1L,
+    y_basis_fun = linear_basis,
+    y_basis_fun_prime = linear_basis_prime,
+    basis = constraint
+  )
R> 
R> set.seed(1)
R> n <- 1e3
R> d <- data.frame(y = 1 + rnorm(n), x = rnorm(n))
R> m <- deeptrafo(y ~ 0 + x, data = d, trafo_options = tfc,
+                 optimizer = optimizer_adam(learning_rate = 1e-2),
+                 latent_distr = "normal")
R> fit(m, batch_size = n, epochs = 5e3, validation_split = NULL,
+      callbacks = list(callback_reduce_lr_on_plateau(monitor = "loss")),
+      verbose = FALSE)
R> abs(unlist(coef(m)) - coef(Lm(y ~ x, data = d)))
     x 
0.0245 
R> 
R> 
R> ## ----alternative-interface, eval=!ATMonly-------------------------------------
R> dord <- data.frame(Y = ordered(sample.int(6, 100, TRUE)),
+                     X = rnorm(100), Z = rnorm(100))
R> ontram(response = ~ Y, intercept = ~ X, shift = ~ 0 + s(Z, df = 3),
+         data = dord)
	 Untrained ordinal outcome deep conditional transformation model

Call:
ontram(response = ~Y, intercept = ~X, shift = ~0 + s(Z, df = 3), 
    data = dord)

Interacting:  Y | X 

Shifting:  ~0 + s(Z, df = 3) 

Shift coefficients:
s(Z, df = 3)1 s(Z, df = 3)2 s(Z, df = 3)3 s(Z, df = 3)4 s(Z, df = 3)5 
      -0.4760       -0.7326       -0.6233       -0.4061       -0.4309 
s(Z, df = 3)6 s(Z, df = 3)7 s(Z, df = 3)8 s(Z, df = 3)9 
      -0.5447        0.6729        0.7376        0.0947 
R> 
R> ## ----word2vec-----------------------------------------------------------------
R> 
R> ### Load embedding
R> embedding_dim <- 300
R> 
R> if (file.exists("word2vec_embd_matrix.RDS")) {
+  
+    embedding_matrix <- readRDS("word2vec_embd_matrix.RDS")
+    vocab_size <- nrow(embedding_matrix)
+  
+  } else {
+  
+    # py_install(packages = "gensim")
+    gensim <- import("gensim")
+    # download embedding
+    # https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing
+    model <- gensim$models$KeyedVectors$load_word2vec_format(
+      "../Data/GoogleNews-vectors-negative300.bin", binary = TRUE)
+  
+    vocab_size <- length(words$word)
+    embedding_matrix <- matrix(0, nrow = vocab_size, ncol = embedding_dim)
+  
+    names_model <- names(model$key_to_index)
+    for (i in 1:vocab_size) {
+      word <- words$word[i]
+      if (word %in% names_model) {
+        embedding_matrix[i, ] <- model[[word]]
+      }
+    }
+    saveRDS(embedding_matrix, file = "word2vec_embd_matrix.RDS")
+  
+  }
R> 
R> ### Shallow
R> w2v_mod <- function(x) x |>
+    layer_embedding(input_dim = vocab_size, output_dim = embedding_dim,
+                    weights = list(embedding_matrix), trainable = FALSE) |>
+    layer_flatten() |>
+    layer_dense(units = 1)
R> 
R> fm_w2v <- action ~ 0 + shallow(texts)
R> m_w2v <- deeptrafo(fm_w2v, data = train,
+                     list_of_deep_models = list(shallow = w2v_mod),
+                     optimizer = optimizer_adam(learning_rate = 1e-5))
R> 
R> dhist <- fit(m_w2v, epochs = 200, validation_split = 0.1, batch_size = 32,
+               callbacks = list(callback_early_stopping(patience = 5)),
+               verbose = FALSE)
R> 
R> bci(m_w2v)
  nll   lwr   upr 
0.523 0.494 0.553 
R> 
R> ### Deep
R> w2v2_mod <- function(x) x |>
+    layer_embedding(input_dim = vocab_size, output_dim = embedding_dim,
+                    weights = list(embedding_matrix), trainable = FALSE) |>
+    layer_conv_1d(filters = 128, kernel_size = 5, activation = "relu") |>
+    layer_max_pooling_1d(pool_size = 5) |>
+    layer_conv_1d(filters = 128, kernel_size = 5, activation = "relu") |>
+    layer_global_max_pooling_1d() |>
+    layer_dense(units = 128, activation = "relu") |>
+    layer_dropout(rate = 0.5) |>
+    layer_dense(units = 1, activation = "sigmoid")
R> 
R> fm_w2v2 <- action ~ 0 + deep(texts)
R> m_w2v2 <- deeptrafo(fm_w2v2, data = train,
+                      list_of_deep_models = list(deep = w2v2_mod),
+                      optimizer = optimizer_adam(learning_rate = 1e-5))
R> 
R> dhist <- fit(m_w2v2, epochs = 200, validation_split = 0.1, batch_size = 32,
+               callbacks = list(callback_early_stopping(patience = 5)),
+               verbose = FALSE)
R> 
R> bci(m_w2v2)
  nll   lwr   upr 
0.510 0.486 0.534 
R> 
R> ## ----large-factor-models------------------------------------------------------
R> set.seed(0)
R> n <- 1e6
R> nlevs <- 1e3
R> X <- factor(sample.int(nlevs, n, TRUE))
R> Y <- (X == 2) - (X == 3) + rnorm(n)
R> d <- data.frame(Y = Y, X = X)
R> m <- LmNN(Y ~ 0 + fac(X), data = d, additional_processor = list(
+    fac = fac_processor), optimizer = optimizer_adam(learning_rate = 1e-2))
R> fit(m, batch_size = 1e4, epochs = 20, validation_split = 0, callbacks = list(
+    callback_early_stopping("loss", patience = 3),
+    callback_reduce_lr_on_plateau("loss", 0.9, 2)), verbose = FALSE)
R> bl <- unlist(coef(m, which = "interacting"))
R> - (unlist(coef(m))[1:5] + bl[1]) / bl[2]
fac(X)1 fac(X)2 fac(X)3 fac(X)4 fac(X)5 
-0.0204  0.9987 -1.0157 -0.0250  0.0477 
R> logLik(m, batch_size = 1e4, convert_fun = \(x) - mean(x))
[1] -1.42
R> 
> proc.time()
   user  system elapsed 
  15138    1511    4131 
